# Kubernetes Security Architecture Guide

**Last Updated:** January 27, 2026

A cloud-agnostic guide for building production-ready Kubernetes clusters with defense-in-depth security, high availability, and disaster recovery. This guide includes industry best practices and lessons learned from real-world production implementations.

## Table of Contents

1. [Overview](#1-overview)
   - [Do You Need Kubernetes?](#do-you-need-kubernetes)
2. [Prerequisites](#2-prerequisites)
   - [Required Tools](#required-tools)
   - [External Services](#external-services)
3. [Network Architecture & Database Layer](#3-network-architecture--database-layer)
   - [Network Design](#network-design)
   - [Database Layer](#database-layer)
4. [Cluster Architecture & Separation](#4-cluster-architecture--separation)
   - [Two-Cluster Design (Recommended)](#two-cluster-design-recommended)
   - [Single Cluster Alternative](#single-cluster-alternative)
   - [Network Policy Implementation](#network-policy-implementation)
5. [Ingress & Traffic Management](#5-ingress--traffic-management)
   - [Load Balancer Architecture](#load-balancer-architecture)
   - [WAF Configuration](#waf-configuration)
   - [Istio Service Mesh](#istio-service-mesh)
6. [Policy Enforcement with Kyverno](#6-policy-enforcement-with-kyverno)
   - [Kyverno Policy Engine](#kyverno-policy-engine)
7. [Continuous Vulnerability & Threat Detection](#7-continuous-vulnerability--threat-detection)
   - [Trivy Operator for Vulnerability Scanning](#trivy-operator-for-vulnerability-scanning)
   - [Falco Runtime Security](#falco-runtime-security)
8. [Secrets Management](#8-secrets-management)
   - [External Secrets Management](#external-secrets-management)
   - [AWS EKS Integration](#aws-eks-integration)
   - [GCP GKE Integration](#gcp-gke-integration)
   - [Azure AKS Integration](#azure-aks-integration)
   - [Secret Rotation](#secret-rotation)
9. [Infrastructure as Code & GitOps](#9-infrastructure-as-code--gitops)
   - [Terraform for Infrastructure](#terraform-for-infrastructure)
   - [ArgoCD for GitOps](#argocd-for-gitops)
10. [Observability & Logging](#10-observability--logging)
    - [Fluentd Log Aggregation](#fluentd-log-aggregation)
    - [Prometheus & Grafana](#prometheus--grafana)
    - [Log Retention & Compliance](#log-retention--compliance)
11. [Identity & Access Management](#11-identity--access-management)
    - [Kubernetes RBAC (Role-Based Access Control)](#kubernetes-rbac)
    - [Workload Identity & Cloud IAM Integration](#workload-identity--cloud-iam-integration)
    - [Cloud IAM Policy Best Practices](#iam-policy-best-practices)
12. [Disaster Recovery](#12-disaster-recovery)
    - [Recovery Strategy](#recovery-strategy)
    - [Recovery Procedure](#recovery-procedure)
    - [Testing & Validation](#testing--validation)
13. [Incident Response](#13-incident-response)
    - [Detection & Initial Response](#detection--initial-response)
    - [Containment & Recovery](#containment--recovery)
    - [Post-Incident](#post-incident)
14. [Attack Scenarios Prevented](#14-attack-scenarios-prevented)
15. [References](#15-references)

## 1. Overview

This guide outlines a production-grade Kubernetes architecture that prioritizes security, reliability, and operational excellence. The patterns are cloud-agnostic and work with managed Kubernetes services (AWS EKS, GCP GKE, Azure AKS) and their respective cloud-native services for networking, databases, secrets management, and observability.

**Target Audience:** Organizations with 50+ engineers, 20+ microservices, and dedicated platform teams. If you're a small team or startup, see "Do You Need Kubernetes?" below - you likely should use serverless or Fargate instead.

**Core Principles:**

- **Defense in Depth**: Multiple security layers from network to runtime
- **Least Privilege**: Minimize blast radius through network isolation and access controls
- **High Availability**: Multi-AZ databases, automatic failover, point-in-time recovery
- **Infrastructure as Code**: Versioned, reproducible infrastructure with Terraform and ArgoCD
- **Separation of Concerns**: Isolated clusters for production workloads vs administrative tooling
- **Operational Excellence**: Accept complexity only when scale demands it

### Do You Need Kubernetes?

**You probably DON'T need Kubernetes if:**

- You have <50 engineers
- You run <20 microservices
- Your traffic is <10M requests/day
- You don't have a dedicated platform/DevOps team (3-5 engineers minimum)
- You're trying to look "cloud-native" but haven't validated the operational cost

**What Kubernetes actually requires:**

- 3-5 dedicated platform engineers to manage it properly
- Expertise in: networking, security, storage, observability, GitOps
- Operational complexity: YAML files, Helm charts, kubectl, service meshes, policy engines
- Debugging: pod evictions, OOMKilled errors, image pull failures, DNS issues, network policies
- Cost: $500-2000+/month for minimal production setup, easily $2000-10000+/month with full observability/security stack

**What to use instead:**

**For Serverless Workloads (Recommended for <50 Engineers)**

- **AWS**: Lambda + API Gateway + RDS Aurora
- **GCP**: Cloud Run + Cloud SQL
- **Azure**: Functions + Azure SQL

**Why serverless is better for small teams:**

- Zero operational overhead (no patching, scaling, YAML)
- Pay only for usage ($0-50/month for most startups, free tier covers <1M requests)
- Infinite scale without configuration
- 1 engineer can manage entire infrastructure
- Deploy in minutes, not weeks

**Trade-offs you should accept:**

- Cold starts (100ms-3s) - acceptable for 95% of APIs
- Stateless only (use managed databases for state)

**For Container Workloads (If You Need WebSockets/Streaming)**

- **AWS**: ECS Fargate + ALB + RDS
- **GCP**: Cloud Run (supports WebSockets) + Cloud SQL
- **Azure**: Container Instances + Azure SQL

**Why Fargate over Kubernetes:**

- No cluster to manage (AWS manages control plane AND workers)
- No Kubernetes complexity (YAML, Helm, kubectl, service mesh)
- Still get containers, load balancing, auto-scaling
- 1/10th the operational complexity of K8s
- $100-500/month vs $500-2000+/month for K8s

**You ACTUALLY need Kubernetes when:**

- You have 50+ microservices with complex inter-service networking
- You need sophisticated service mesh (mTLS between hundreds of services)
- You're cost-optimizing at massive scale (spot instances, bin packing, multi-tenancy)
- You have a dedicated platform team (3-5+ engineers)
- You run ML workloads requiring GPU orchestration
- You need multi-tenancy isolation for SaaS products
- You're at "Spotify scale" (not "we watched a KubeCon talk" scale)

**Reality check:** Kubernetes killed more startups than server crashes ever did. A $50/month Fargate container can handle millions of requests. Your startup will run out of runway debugging networking issues long before you need horizontal pod autoscaling.

**Detailed Monthly Cost Breakdown:**

**Serverless Stack (Lambda/Cloud Run):**

- Compute (Lambda/Cloud Run): $0-30 (free tier covers most MVPs, ~$20-30 for 5M requests)
- API Gateway: $3.50 per million requests (~$10-20 for typical usage)
- Managed Database (smallest tier): $15-50
- Secrets Manager: $0.40 per secret (~$2-5)
- **Total: $30-100/month**

**Fargate Stack:**

- Fargate tasks (2-3 for HA, 0.5 vCPU, 1GB RAM each): $30-45
- Application Load Balancer: $16-25
- Managed Database (small): $50-150
- Secrets Manager: $2-5
- NAT Gateway (if private subnets): $32-45
- **Total: $130-270/month (public subnets) or $160-315/month (private subnets)**

**Kubernetes Minimal Production:**

- EKS/GKE/AKS Control Plane: $73
- Worker Nodes (3 t3.medium instances): $150-200
- NAT Gateway (3 AZ): $100-135
- Load Balancer: $25-40
- Managed Database: $50-200
- Secrets Manager: $5-10
- **Total: $400-660/month (before observability stack)**

**Kubernetes Full Production (with this guide's architecture):**

- Above base infrastructure: $400-660
- Istio service mesh overhead: +$50-100 (additional CPU/memory)
- Prometheus + Grafana: +$30-60 (storage, retention)
- Fluentd + centralized logging: +$50-200 (log volume dependent)
- ArgoCD cluster (separate admin cluster): +$200-300
- Trivy Operator scanning: +$20-40
- Additional tooling (Kyverno, external-secrets): +$30-50
- **Total: $780-1410/month for full stack**
- **Realistic production with traffic: $1500-3000+/month**

**If you're still reading, you've validated you actually need Kubernetes. This guide is for you.**

## 2. Prerequisites

### Required Tools

**Infrastructure as Code:**

- [Terraform](https://www.terraform.io/) - Infrastructure provisioning and management
- [Helm](https://helm.sh/) - Kubernetes package manager
- [ArgoCD](https://argo-cd.readthedocs.io/) - GitOps continuous delivery for Kubernetes

**Security & Policy:**

- [Kyverno](https://github.com/kyverno/kyverno) - Kubernetes-native policy engine
- [Trivy Operator](https://github.com/aquasecurity/trivy-operator) - Continuous vulnerability scanning
- [Istio](https://github.com/istio/istio) - Service mesh for mTLS and traffic management
- [Falco](https://github.com/falcosecurity/falco) - Runtime threat detection (optional - see analysis in Section 7)

**Observability:**

- [Prometheus](https://prometheus.io/) - Metrics collection and monitoring
- [Grafana](https://grafana.com/) - Visualization and dashboards
- [Fluentd](https://github.com/fluent/fluentd) - Log collection and forwarding

### External Services

Cloud-agnostic service options for Kubernetes, databases, secrets, logging, and load balancing.

| Service Category                     | AWS                              | GCP                                | Azure                           | Self-Hosted / Open Source |
| ------------------------------------ | -------------------------------- | ---------------------------------- | ------------------------------- | ------------------------- |
| **Managed Kubernetes** (recommended) | Elastic Kubernetes Service (EKS) | Google Kubernetes Engine (GKE)     | Azure Kubernetes Service (AKS)  | -                         |
| **Managed Databases** (required)     | RDS (PostgreSQL, MySQL, Aurora)  | Cloud SQL                          | Database for PostgreSQL/MySQL   | -                         |
| **Secrets Management** (required)    | Secrets Manager                  | Secret Manager                     | Key Vault                       | HashiCorp Vault           |
| **Logging & SIEM** (required)        | CloudWatch Logs                  | Cloud Logging                      | Monitor                         | Splunk, ELK Stack, Loki   |
| **Load Balancing & WAF**             | ALB + AWS WAF                    | Cloud Load Balancing + Cloud Armor | Application Gateway + Azure WAF | -                         |

**Notes:**

- **Managed Kubernetes**: Strongly recommended over self-hosted - reduces operational burden and improves security
- **Managed Databases**: Required for production - never run databases in Kubernetes for production workloads
- **Secrets Management**: Required for secure credential storage and rotation
- **Load Balancing & WAF**: Essential for edge security and DDoS protection

## 3. Network Architecture & Database Layer

Design secure network topology with proper isolation and managed databases for production resilience.

### Network Design

Choose between private and public subnets based on your security requirements and budget constraints.

| Aspect                 | Private Subnets                               | Public Subnets                          |
| ---------------------- | --------------------------------------------- | --------------------------------------- |
| **Worker Node Access** | No direct internet access                     | Public IP addresses                     |
| **Egress Method**      | NAT Gateway required                          | Direct egress                           |
| **Monthly Cost**       | $32-45 per AZ + data transfer fees            | $0 (no NAT Gateway)                     |
| **Multi-AZ Cost**      | 3 AZs = 3x NAT Gateway fees (~$100-135/month) | $0                                      |
| **Security Level**     | Excellent - nodes fully isolated              | Good - requires strict security groups  |
| **Attack Surface**     | Minimal - no public IPs                       | Higher - nodes have public IPs          |
| **Best For**           | Production environments                       | Budget-constrained or non-critical apps |

**Recommendation:** Use private subnets with NAT Gateway for production - the cost is negligible compared to security benefits.

**Public Subnet Configuration** (if chosen):

- Worker nodes in public subnets with strict security groups
- Allow only: ALB traffic, specific admin IPs/VPN
- Block all other inbound traffic
- **Risk**: Requires careful security group configuration to prevent exposure

**Multi-AZ Design** (applies to both):

- Distribute worker nodes across 3 AZs minimum
- Managed Kubernetes control plane automatically multi-AZ
- ALB/NLB automatically span AZs

**Admin Access**:

- VPN (AWS Client VPN, GCP Cloud VPN, Azure VPN Gateway) - recommended
- Bastion host alternative (hardened VM, restricted IPs)
- Admin ALB restricted to VPN range or bastion IP only
- Never expose Kubernetes API or admin tools to 0.0.0.0/0

### Database Layer

**Never run databases in Kubernetes for production workloads.**

Use managed cloud databases instead:

- AWS RDS (PostgreSQL, Aurora)
- GCP Cloud SQL
- Azure Database for PostgreSQL

**Why managed databases:**

Kubernetes is designed for stateless applications. Running databases in Kubernetes introduces operational complexity:

- Persistent storage management across node failures
- Manual backup and recovery procedures
- Complex replication configuration
- Database lifecycle tightly coupled to cluster lifecycle

Managed database services provide:

- Automated backups with point-in-time recovery
- Multi-AZ deployment with automatic failover (60-120 seconds)
- Automated patching and maintenance windows
- Separation of database operations from Kubernetes operations

**Network Architecture:**

- Deploy databases in **private subnets** (no internet access)
- Security groups allow connections **only from Kubernetes worker nodes**
- All database traffic stays within VPC

**Connection Pattern:**

Applications running in Kubernetes retrieve database credentials from cloud secrets managers using Secrets Store CSI Driver:

| Cloud Provider | Integration                                           |
| -------------- | ----------------------------------------------------- |
| **AWS EKS**    | Secrets Store CSI Driver + AWS Secrets Manager        |
| **GCP GKE**    | Workload Identity + Secret Manager                    |
| **Azure AKS**  | Azure Key Vault Provider for Secrets Store CSI Driver |

Credentials are synced from the external vault into Kubernetes secrets, then injected into pods as environment variables. This keeps credentials centralized in the cloud provider's secrets manager (not in Kubernetes native Secrets).

## 4. Cluster Architecture & Separation

Isolate production workloads from administrative tooling using separate Kubernetes clusters.

### Two-Cluster Design (Recommended)

**Production Cluster**:

- Customer-facing applications and services
- Istio for mTLS, Kyverno for policy enforcement
- Falco for runtime monitoring, Trivy Operator for vulnerability scanning
- Exposed via customer-facing ALB with WAF

**Admin Cluster**:

- ArgoCD for GitOps deployments to production cluster
- Prometheus for metrics collection, Grafana for dashboards
- Admin ALB restricted to VPN range or bastion IP only
- No public internet access

**Why separate clusters**:

- Production compromise doesn't affect deployment capability or observability
- Production pods cannot access ArgoCD to modify infrastructure
- Clear separation of duties for compliance (SOC2, ISO 27001)
- Limits blast radius - attackers in production can't pivot to admin tools

### Single Cluster Alternative

Use Kubernetes namespaces with strict NetworkPolicies if cost is primary constraint.

**Mitigations required**:

- Isolate admin namespace with NetworkPolicies
- Kyverno policies to prevent production pods from accessing admin resources
- Admin ingress still restricted to VPN/bastion only
- Only recommended for non-critical applications or small teams

**Recommendation**: Use separate clusters for production - minimal overhead with managed Kubernetes, significant security benefit.

### Network Policy Implementation

Without NetworkPolicies, namespace isolation is convention only. Apply these two policies to enforce separation:

**Default deny all ingress (apply to each namespace):**

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-ingress
  namespace: production
spec:
  podSelector: {}
  policyTypes:
    - Ingress
```

**Allow traffic from Istio gateway to your apps:**

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-from-gateway
  namespace: production
spec:
  podSelector:
    matchLabels:
      app: api
  ingress:
    - from:
        - namespaceSelector:
            matchLabels:
              name: istio-system
      ports:
        - protocol: TCP
          port: 8080
```

Apply default-deny first, then explicitly allow required traffic. Test with: `kubectl exec -it pod-name -- curl http://service.namespace.svc.cluster.local`

## 5. Ingress & Traffic Management

Configure load balancers, WAF, and service mesh to secure and route traffic to appropriate services.

### Load Balancer Architecture

**Customer-Facing ALB**:

- Public-facing load balancer for customer APIs and web services
- TLS termination with managed certificates (ACM, GCP Managed Certificates, Azure Key Vault)
- Routes to Istio ingress gateway in production cluster
- WAF enabled (AWS WAF, GCP Cloud Armor, Azure WAF)

**Admin ALB**:

- Separate load balancer for admin tools (ArgoCD, Grafana)
- Security group restricted to VPN IP range or bastion IP only
- Routes to admin cluster services
- Never accessible from public internet (0.0.0.0/0)

### WAF Configuration

Deploy Web Application Firewall at load balancer to filter malicious traffic:

- Protect against OWASP Top 10 (SQL injection, XSS, etc.)
- Rate limiting for DDoS mitigation
- Block known malicious IPs and bot traffic

### Istio Service Mesh

**Mutual TLS (mTLS)**:

- Automatic mTLS encryption between all pods
- Prevents man-in-the-middle attacks on internal traffic
- Zero configuration required after Istio installation
- Automatic certificate rotation

**Traffic Management**:

- Intelligent routing based on headers, weights, or conditions
- Canary deployments: Route 5% of traffic to new version
- A/B testing: Route specific users to experimental features
- Circuit breaking: Fail fast when backend services are unhealthy

**Observability**:

- Distributed tracing with Jaeger or Zipkin
- Service-to-service metrics (latency, error rates)
- Visualize traffic flow with Kiali dashboard

## 6. Policy Enforcement with Kyverno

Enforce security policies at deployment time to prevent misconfigurations and ensure compliance.

### Kyverno Policy Engine

Deploy [Kyverno](https://github.com/kyverno/kyverno) for Kubernetes-native policy enforcement without learning a new language.

**Essential Security Policies:**

Kyverno enforces these policies at pod deployment to prevent security misconfigurations:

1. **Require Resource Limits** - Prevents resource exhaustion by requiring CPU/memory limits
2. **Block Privileged Containers** - Prevents privilege escalation attacks
3. **Require Non-Root User** - Ensures containers run as non-root user
4. **Verify Image Signatures** - Validates images are signed with Cosign

---

**Example Policy: Require Resource Limits**

```yaml
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: require-resource-limits
spec:
  validationFailureAction: enforce
  rules:
    - name: check-resources
      match:
        resources:
          kinds:
            - Pod
      validate:
        message: "CPU and memory limits required"
        pattern:
          spec:
            containers:
              - resources:
                  limits:
                    memory: "?*"
                    cpu: "?*"
```

---

**Example Policy: Block Privileged Containers**

```yaml
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: disallow-privileged
spec:
  validationFailureAction: enforce
  rules:
    - name: check-privileged
      match:
        resources:
          kinds:
            - Pod
      validate:
        message: "Privileged mode is not allowed"
        pattern:
          spec:
            containers:
              - securityContext:
                  privileged: false
```

---

**Example Policy: Require Non-Root Containers**

```yaml
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: require-non-root
spec:
  validationFailureAction: enforce
  rules:
    - name: check-runAsNonRoot
      match:
        resources:
          kinds:
            - Pod
      validate:
        message: "Containers must run as non-root user"
        pattern:
          spec:
            securityContext:
              runAsNonRoot: true
```

---

**Example Policy: Verify Image Signatures**

Requires [Cosign](https://github.com/sigstore/cosign) for signing container images.

```yaml
apiVersion: kyverno.io/v1
kind: ClusterPolicy
metadata:
  name: verify-image-signature
spec:
  validationFailureAction: enforce
  rules:
    - name: check-signature
      match:
        resources:
          kinds:
            - Pod
      verifyImages:
        - imageReferences:
            - "*"
          attestors:
            - count: 1
              entries:
                - keys:
                    publicKeys: |-
                      -----BEGIN PUBLIC KEY-----
                      ...your public key...
                      -----END PUBLIC KEY-----
```

---

**Additional Security Policies:**

Implement these additional policies for comprehensive security:

- Block hostNetwork, hostPID, hostIPC usage
- Require pod security labels
- Enforce image registry allowlist (only approved registries)
- Validate required security contexts
- Require read-only root filesystem where possible
- Block dangerous capabilities
- Enforce distroless or minimal base images (no package managers)

**Deployment Workflow:**

Kyverno runs as admission controller, validates policies before pods are created, blocks non-compliant workloads automatically.

## 7. Continuous Vulnerability & Threat Detection

Monitor running workloads for vulnerabilities and detect runtime threats in real-time.

### Trivy Operator for Vulnerability Scanning

Deploy [Trivy Operator](https://github.com/aquasecurity/trivy-operator) for continuous security scanning in Kubernetes.

**What it scans**:

- Container images for OS and application vulnerabilities
- Kubernetes configuration for security misconfigurations
- Infrastructure as Code (IaC) files for compliance issues
- SBOM generation for all running images

**How it works**:

- Runs as Kubernetes operator (continuously scans cluster)
- Scans new images automatically when pods are deployed
- Stores results as Kubernetes custom resources (VulnerabilityReports, ConfigAuditReports)
- Integrates with Prometheus for alerting on critical vulnerabilities

**Scanning Strategy**:

- Daily automated scans of all container images in cluster
- Scan on new pod deployment
- Generate vulnerability reports as Kubernetes custom resources
- Alert on HIGH and CRITICAL vulnerabilities with available fixes

**Reporting & Integration**:

- Export vulnerability reports to Fluentd
- Export scan results to Prometheus for metrics
- Forward to external SIEM (Splunk, ELK Stack, cloud logging)
- Visualize in Grafana dashboards
- Store reports in object storage (S3/GCS/Azure Blob)
- Track vulnerability remediation over time

**Automated Response**:

- Trigger alerts when new CVEs discovered in running images
- Optionally trigger automated image rebuilds via ArgoCD/CI pipeline
- Update deployments with patched images

### Falco Runtime Security (Optional)

[Falco](https://github.com/falcosecurity/falco) provides real-time runtime threat detection in containers, but should be carefully evaluated based on your security architecture's existing preventive controls.

**What Falco detects**:

- Shell spawned in container (potential breakout attempt)
- Unexpected process execution in containers
- Sensitive file access (/etc/shadow, SSH keys, credentials)
- File system modifications in read-only paths
- Unexpected network connections
- Network connections to unexpected destinations
- Privilege escalation attempts
- Container processes accessing host filesystem
- Suspicious system calls

**Trade-Off Analysis: When Falco Adds Limited Value**

If your architecture already implements comprehensive preventive controls, Falco becomes largely redundant and may not justify its costs:

**1. Redundancy with Preventive Controls**

When you have all of these in place, an attacker who compromises a container has almost nothing they can execute:

- **Distroless images**: No shell, no package managers (apt/yum/apk), no binaries beyond application code
- **Non-root enforcement**: Attacker cannot write to most filesystem locations or escalate privileges
- **Read-only root filesystem**: Even if attacker finds writable location, filesystem is immutable
- **Restrictive NetworkPolicies**: Default-deny egress blocks data exfiltration and C2 communications
- **Istio mTLS**: Service-to-service communication locked down, preventing lateral movement

**Result**: Falco would detect activities that your preventive controls already make impossible. An attacker literally has no shell to spawn, no tools to download, nowhere to write files, and no network path to exfiltrate data.

**2. Attack Surface Expansion**

Falco introduces its own security risks:

- **Privileged access**: Runs as privileged DaemonSet with kernel-level access via eBPF/kernel module
- **High-value target**: Compromise Falco = visibility into ALL containers on the node
- **Supply chain risk**: Another container image to scan, patch, and manage CVEs for
- **Complexity**: Additional failure mode and potential misconfiguration risks

**3. Performance Overhead**

- Intercepts syscalls across all pods on every node
- CPU overhead scales with cluster activity (more pods = more overhead)
- Memory overhead for buffering and processing events
- Though marketed as "zero impact," production clusters report 2-5% CPU overhead at scale

**4. Operational Complexity**

- Requires tuning rules to reduce false positives
- Alert fatigue from noisy detections
- Another system to update, monitor, and maintain
- Team needs expertise to interpret Falco alerts and respond appropriately

**When Falco IS Worth Deploying**

Falco provides valuable detective capabilities in these scenarios:

1. **Weak preventive controls**: If you cannot enforce distroless, non-root, read-only filesystem, or network policies
2. **Zero-day detection**: Catches novel attacks exploiting application logic bugs that don't need external tools
3. **Insider threat**: Malicious code deployed through legitimate CI/CD or by insiders with access
4. **Compliance mandate**: Some frameworks explicitly require runtime monitoring (though alternatives may satisfy this)
5. **"Assume breach" philosophy**: If your threat model assumes preventive controls will fail

**Alternative Approaches Without Falco**

These provide overlapping detection capabilities with lower overhead:

1. **Kubernetes audit logs**: Track suspicious API activity (pod exec attempts, secret access)
2. **Prometheus/Grafana anomalies**: Monitor pod restarts, network patterns, resource spikes
3. **Cloud-native logging**: CloudWatch/Cloud Logging/Azure Monitor for centralized audit trails
4. **Trivy Operator**: Continuous vulnerability scanning catches exploitable CVEs before attackers can use them
5. **Periodic penetration testing**: Red team exercises validate your preventive controls work as intended

**Recommendation**

For architectures with comprehensive preventive controls (distroless, non-root, read-only FS, restrictive NetworkPolicies, Istio mTLS), **Falco is optional and likely not worth the operational/security trade-offs**.

Document your decision with a risk acceptance statement: "We accept the risk of undetected runtime threats because our preventive controls make successful runtime attacks highly improbable, and the operational/security costs of Falco outweigh the marginal detection benefit."

**If deploying Falco despite preventive controls**, recognize you're optimizing for defense-in-depth at the cost of complexity.

**Deployment** (if chosen):

- Deploy as DaemonSet (runs on every node)
- Uses eBPF or kernel module to intercept system calls
- Rules are customizable for your environment

**Alert Configuration** (if chosen):

- Forward alerts to Fluentd, then to SIEM
- Integrate with Slack/PagerDuty for real-time notifications
- Configure severity levels (info, warning, critical)
- Alert on critical events only to reduce noise

## 8. Secrets Management

Store secrets in external vault services and inject them into Kubernetes pods securely with modern lifecycle management practices.

### External Secrets Management

**Never store secrets in**:

- Kubernetes Secrets (base64 encoded, not encrypted at rest by default)
- ConfigMaps
- Environment variables in Dockerfiles
- Git repositories

**Always store secrets in**:

- AWS Secrets Manager, GCP Secret Manager, Azure Key Vault
- HashiCorp Vault
- External secrets management with encryption, access control, audit logging

### AWS EKS Integration

- Secrets Store CSI Driver with AWS Secrets Manager provider
- IAM Roles for Service Accounts (IRSA) for authentication
- Secrets mounted as volumes (not environment variables for sensitive data)

### GCP GKE Integration

- Workload Identity for pod authentication to Secret Manager
- GCP Secret Manager CSI Driver
- Secrets mounted as volumes

### Azure AKS Integration

- Azure Key Vault Provider for Secrets Store CSI Driver
- Managed identities for pod authentication
- Secrets mounted as volumes

### Secret Rotation

**Modern best practices** (NIST, CNCF): Routine rotation no longer recommended - focus on preventing exposure.

**Rotate only when**:

- Secrets confirmed or suspected compromised
- Employee with access leaves organization
- Compliance requirements mandate rotation

**Better security approach**:

- Use short-lived credentials (IAM roles, workload identity)
- Implement proper access controls and audit logging
- Monitor for unauthorized access attempts

## 9. Infrastructure as Code & GitOps

Manage infrastructure and applications as versioned code for reproducibility and automation.

### Terraform for Infrastructure

Use Terraform to provision and manage all cloud infrastructure as code.

**What Terraform manages**:

- VPC, subnets, security groups, route tables
- Kubernetes clusters (EKS, GKE, AKS)
- Load balancers (ALB, NLB)
- Databases (RDS, Cloud SQL, Azure Database)
- IAM roles, service accounts, policies
- Secrets managers, logging infrastructure

**Version Control & State**:

- Store Terraform code in Git repository
- Use semantic versioning for infrastructure releases
- Require pull request reviews for infrastructure changes
- Store Terraform state in remote backend (S3, GCS, Azure Blob)
- Enable state locking to prevent concurrent modifications
- Encrypt state at rest and maintain regular backups

**Workspace Strategies:**

**Option 1: Workspace-Per-Environment** (Smaller teams):

- Single codebase with `terraform workspace` for dev/staging/production
- Advantages: Less duplication, shared modules, easy to keep in sync
- Disadvantages: Shared state file, risk of accidental cross-environment changes

**Option 2: Separate State Files** (Recommended for production):

- Separate directories for each environment with independent state files
- Advantages: Complete isolation, environment-specific access controls, no cross-environment risk
- Disadvantages: More code duplication (mitigated by modules)

**Recommendation**: Use separate state files for production (`environments/production/`), workspaces acceptable for dev/staging.

**Module Organization:**

Organize into reusable modules to reduce duplication:

```
modules/
├── vpc/          # VPC, subnets, NAT gateways
├── eks-cluster/  # EKS cluster, node groups, IRSA
└── rds-postgres/ # RDS instance, subnet group, security group

environments/
├── dev/main.tf
├── staging/main.tf
└── production/main.tf  # References modules
```

**Sensitive Data Handling:**

- Mark sensitive outputs: `sensitive = true` (prevents console display, still in state)
- Encrypt state files: Use KMS keys for S3/GCS/Azure backend encryption
- Restrict state access: Least-privilege IAM policies for state bucket/table
- Never commit secrets: Use vault references, not hardcoded values

**Drift Detection:**

Infrastructure drift occurs when manual changes bypass Terraform:

```bash
# Detect drift
terraform plan -detailed-exitcode  # Exit code 2 = drift detected

# Automated daily drift detection in CI/CD
- name: Drift Detection
  run: terraform plan -no-color -detailed-exitcode
  continue-on-error: true  # Alert but don't fail
```

**Remediation**: Import manual changes (`terraform import`), revert with `terraform apply`, or update Terraform to match reality.

**Team Collaboration:**

**Pull Request Workflow:**

1. Developer creates branch and modifies `.tf` files
2. CI/CD runs validation: `terraform fmt -check`, `terraform validate`, `terraform plan`
3. Team reviews plan output in PR comments
4. Approval: 1 for dev/staging, 2 for production (1 from security)
5. Merge triggers `terraform apply` (auto for dev/staging, manual for production)

**RBAC**: Developers can plan (read-only), SRE/DevOps can apply with approval, security team has audit access.

**Cost Estimation:**

Use Infracost to preview cost impact before applying:

```yaml
- name: Run Infracost
  run: infracost breakdown --path . --format json
# Example output: Monthly cost change: +$653 (m5.xlarge + db.r5.2xlarge)
```

**Testing:**

- **Pre-apply**: `terraform validate`, `tfsec` (security scan), `checkov` (policy-as-code)
- **Preview environments**: Create temporary workspace to test major changes
- **Integration tests**: Terratest for module validation

**Provider Version Management:**

```hcl
terraform {
  required_version = ">= 1.5.0"
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"  # Allow 5.x, block 6.0 breaking changes
    }
  }
}
```

Upgrade process: Test in dev → review changelog → staging → production with maintenance window.

**Disaster Recovery:**

**State File Corruption/Deletion:**

1. Enable S3 versioning on state bucket (required)
2. Automated daily backups to separate bucket
3. Cross-region replication for critical state
4. Recovery: Restore from version history or backup

**Accidental Destroy:**

- Prevention: `lifecycle { prevent_destroy = true }` on critical resources
- Recovery: Restore from backup, re-import resources with `terraform import`

**Backup Automation:**

```bash
#!/bin/bash
# Daily state backup
for ENV in dev staging production; do
  aws s3 cp s3://terraform-state/$ENV/terraform.tfstate \
    s3://terraform-state-backup/$ENV/terraform.tfstate.$(date +%Y%m%d)
done
```

**Key Takeaways:**

- Use separate state files for production with strict access controls
- Implement automated drift detection to catch manual changes
- Require code reviews and approvals for all infrastructure changes
- Maintain comprehensive state file backups with version history
- Practice disaster recovery procedures quarterly

### ArgoCD for GitOps

Deploy ArgoCD in admin cluster to manage application deployments to production cluster.

**GitOps Workflow**:

- Application manifests (Kubernetes YAML, Helm charts) stored in Git
- ArgoCD monitors Git repository for changes
- Automatically syncs changes to production cluster
- Git is single source of truth for cluster state

**Benefits**:

- Declarative infrastructure - desired state defined in Git
- Complete audit trail - all changes tracked in Git history
- Easy rollback - revert Git commit to roll back deployment
- Automated deployment - no manual kubectl commands

**Security**:

- ArgoCD runs in separate admin cluster (isolated from production)
- RBAC controls which teams can deploy to which namespaces
- Require signed Git commits for production deployments
- Admin access restricted to VPN/bastion

## 10. Observability & Logging

Collect, aggregate, and export logs and metrics for monitoring, debugging, and compliance.

### Fluentd Log Aggregation

Deploy Fluentd as DaemonSet to collect and export logs from all cluster components.

**Log Sources**:

- Container logs (stdout/stderr from all pods)
- Kubernetes audit logs (API server events)
- Node system logs
- Application logs

**Export Destinations**:

- External SIEM: Splunk, ELK Stack
- Cloud logging: AWS CloudWatch Logs, GCP Cloud Logging, Azure Monitor
- Long-term storage: S3, GCS, Azure Blob for compliance

**Structured Logging**:

- Use JSON format for application logs
- Include correlation IDs, user IDs, timestamps
- Enables easy parsing and filtering in SIEM

### Prometheus & Grafana

Deploy in admin cluster for metrics collection and visualization.

**Prometheus** collects metrics from production cluster:

- Pod resource usage (CPU, memory, network)
- HTTP request rates, latency, error rates
- Database connection pool usage
- Infrastructure health (node status, disk usage)

**Grafana** provides dashboards and alerting:

- Visualize Prometheus metrics
- Alert on threshold violations (high CPU, pod crashes, error rate spikes)
- Track security metrics (Kyverno violations, Falco alerts, Trivy vulnerabilities)
- Accessible only via admin ALB (VPN/bastion restricted)

### Log Retention & Compliance

**Hot Storage** (30 days):

- AWS CloudWatch Logs, GCP Cloud Logging, Azure Monitor
- Fast access for debugging and incident response
- Real-time searching and alerting

**Cold Storage** (Multi-Year for Compliance):

- S3 Glacier, GCS Coldline/Archive, Azure Archive
- Compressed logs for regulatory compliance

**Retention Requirements by Compliance Standard:**

| Compliance Standard | Retention Period | Scope                                              |
| ------------------- | ---------------- | -------------------------------------------------- |
| **SOC2**            | 1-7 years        | Audit logs, access logs, security events           |
| **ISO 27001**       | 1-3 years        | Security logs, incident records                    |
| **HIPAA**           | 6 years          | PHI access logs, audit trails                      |
| **GDPR**            | 1-3 years        | Personal data access logs (with right to deletion) |

**Archive Process**:

1. Export from hot storage after 30 days
2. Compress (gzip, zstd)
3. Upload to cold storage with lifecycle policies
4. Delete from hot storage

## 11. Identity & Access Management

Implement least-privilege access control through Kubernetes RBAC and cloud provider IAM integration to minimize blast radius of compromised credentials.

### Kubernetes RBAC

**ServiceAccount Configuration**:

- Create dedicated ServiceAccount per application (not `default`)
- Set `automountServiceAccountToken: false` unless pod needs Kubernetes API access
- Use namespace-scoped Roles (not ClusterRoles) for applications
- Grant minimal permissions: `get` only, avoid `list`, `watch`, `*` verbs

**RBAC Example**:

```yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: api-server-sa
  namespace: production
automountServiceAccountToken: false
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: api-server-role
  namespace: production
rules:
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: api-server-binding
  namespace: production
subjects:
  - kind: ServiceAccount
    name: api-server-sa
roleRef:
  kind: Role
  name: api-server-role
  apiGroup: rbac.authorization.k8s.io
```

**Pod Security Context**:

```yaml
spec:
  serviceAccountName: api-server-sa
  securityContext:
    runAsNonRoot: true
    runAsUser: 1000
  containers:
    - name: app
      securityContext:
        allowPrivilegeEscalation: false
        readOnlyRootFilesystem: true
        capabilities:
          drop: ["ALL"]
```

### Workload Identity & Cloud IAM Integration

Allow pods to assume cloud IAM roles without storing credentials. This eliminates the need for service account keys and provides automatic credential rotation.

**Key Differences by Cloud Provider:**

| Feature                 | AWS EKS (IRSA)               | GCP GKE (Workload Identity)      | Azure AKS (Workload Identity)       |
| ----------------------- | ---------------------------- | -------------------------------- | ----------------------------------- |
| **Setup Complexity**    | Medium (OIDC provider)       | Medium (Workload pool)           | Medium (Federated credential)       |
| **Auth Method**         | OIDC federation              | Workload Identity binding        | Managed identity federation         |
| **Credential Lifetime** | 15 min (auto-refresh)        | 60 min (auto-refresh)            | Variable (auto-refresh)             |
| **Annotation Required** | `eks.amazonaws.com/role-arn` | `iam.gke.io/gcp-service-account` | `azure.workload.identity/client-id` |
| **IAM Role Type**       | IAM Role with trust policy   | GCP Service Account              | Azure Managed Identity              |

---

**AWS EKS - IRSA (IAM Roles for Service Accounts):**

Enable pods to assume IAM roles using OIDC federation.

```bash
# Step 1: Enable OIDC provider on cluster
eksctl utils associate-iam-oidc-provider --cluster=production-cluster --approve

# Step 2: Create IAM role with trust policy for ServiceAccount
# (Attach least-privilege IAM policy with specific resources only)

# Step 3: Annotate Kubernetes ServiceAccount
kubectl annotate serviceaccount api-server-sa \
  -n production \
  eks.amazonaws.com/role-arn=arn:aws:iam::ACCOUNT_ID:role/api-server-role
```

**Key Points:**

- OIDC provider creates trust relationship between EKS and IAM
- Pods automatically receive temporary credentials via AWS STS
- No credentials stored in cluster or environment variables

---

**GCP GKE - Workload Identity:**

Bind Kubernetes ServiceAccounts to GCP Service Accounts for seamless authentication.

```bash
# Step 1: Enable Workload Identity on cluster
gcloud container clusters update production-cluster \
  --workload-pool=PROJECT_ID.svc.id.goog

# Step 2: Create GCP service account
gcloud iam service-accounts create api-server-sa

# Step 3: Grant IAM permissions to GCP service account
gcloud projects add-iam-policy-binding PROJECT_ID \
  --member="serviceAccount:api-server-sa@PROJECT_ID.iam.gserviceaccount.com" \
  --role="roles/secretmanager.secretAccessor"

# Step 4: Bind Kubernetes SA to GCP SA
gcloud iam service-accounts add-iam-policy-binding \
  api-server-sa@PROJECT_ID.iam.gserviceaccount.com \
  --role=roles/iam.workloadIdentityUser \
  --member="serviceAccount:PROJECT_ID.svc.id.goog[production/api-server-sa]"

# Step 5: Annotate Kubernetes ServiceAccount
kubectl annotate serviceaccount api-server-sa \
  -n production \
  iam.gke.io/gcp-service-account=api-server-sa@PROJECT_ID.iam.gserviceaccount.com
```

**Key Points:**

- Workload pool establishes trust between GKE and GCP IAM
- Binding creates 1:1 relationship between K8s SA and GCP SA
- Credentials automatically injected into pod environment

---

**Azure AKS - Workload Identity:**

Use managed identities with federated credentials for pod authentication.

```bash
# Step 1: Enable Workload Identity on cluster
az aks update \
  --resource-group production-rg \
  --name production-cluster \
  --enable-workload-identity

# Step 2: Create Azure managed identity
az identity create --name api-server-identity --resource-group production-rg

# Step 3: Grant permissions to managed identity
az role assignment create \
  --assignee CLIENT_ID \
  --role "Key Vault Secrets User" \
  --scope /subscriptions/SUB_ID/resourceGroups/production-rg/providers/Microsoft.KeyVault/vaults/prod-keyvault

# Step 4: Create federated credential for K8s ServiceAccount
az identity federated-credential create \
  --name api-server-federated \
  --identity-name api-server-identity \
  --resource-group production-rg \
  --issuer OIDC_ISSUER_URL \
  --subject system:serviceaccount:production:api-server-sa

# Step 5: Annotate Kubernetes ServiceAccount
kubectl annotate serviceaccount api-server-sa \
  -n production \
  azure.workload.identity/client-id=CLIENT_ID
```

**Key Points:**

- Federated credential links managed identity to K8s ServiceAccount
- Azure automatically handles token exchange and renewal
- Works with Azure AD-integrated resources (Key Vault, Storage, etc.)

### IAM Policy Best Practices

**Least Privilege**:

- Grant only required actions (avoid `*` wildcards)
- Restrict to specific resources (exact ARNs, paths, buckets)
- One IAM role per application (never share)
- Separate roles for dev/staging/production

**Example AWS Policy**:

```json
{
  "Version": "2012-10-17",
  "Statement": [
    {
      "Effect": "Allow",
      "Action": "secretsmanager:GetSecretValue",
      "Resource": "arn:aws:secretsmanager:region:account:secret:prod/api/*"
    }
  ]
}
```

**Verification**:

```bash
# Test RBAC permissions
kubectl auth can-i get secrets \
  --as=system:serviceaccount:production:api-server-sa -n production

# Audit IAM usage
# AWS: CloudTrail logs for AssumeRoleWithWebIdentity
# GCP: Cloud Audit Logs for service account usage
# Azure: Activity logs for managed identity auth
```

## 12. Disaster Recovery

Complete environment recovery through infrastructure as code, database backups, and GitOps.

### Recovery Strategy

All critical components can be recreated from code and backups:

**Infrastructure** (Terraform):

- Terraform state stored in remote backend (S3, GCS, Azure Blob)
- Run `terraform apply` to recreate VPC, clusters, load balancers, databases in new region
- Infrastructure recreated from code in 30-60 minutes

**Databases** (Managed Services):

- Restore from automated snapshots or point-in-time recovery (15-30 minutes)
- Update Kubernetes secrets with new database endpoint after restore

**Applications** (ArgoCD):

- Point ArgoCD at Git repository
- ArgoCD automatically deploys all applications to new cluster
- Cluster state matches Git repository in 10-20 minutes

### Recovery Procedure

Complete disaster recovery steps:

1. Provision infrastructure with Terraform (30-60 minutes)
2. Restore databases from snapshots to new instances (15-30 minutes)
3. **Emergency Secret Rotation** (if compromise suspected): Rotate all secrets in vault (5-10 minutes)
   - Generate new database passwords, API keys, service account credentials
   - Update in external vault (AWS Secrets Manager, GCP Secret Manager, Azure Key Vault)
   - Secrets Store CSI Driver automatically syncs new secrets to pods on restart
4. Deploy ArgoCD to new admin cluster (5 minutes)
5. ArgoCD syncs all applications to new production cluster (10-20 minutes)
6. Update DNS to point to new load balancers (5 minutes + TTL propagation)

**Validation**: Check pod status (`kubectl get pods`), authentication logs, database connectivity.

**Total RTO**: 60-120 minutes  
**RPO**: 5 minutes (database point-in-time recovery)

### Testing & Validation

**Disaster recovery drills**:

- Perform quarterly in non-production environment
- Test infrastructure recreation with Terraform
- Verify database restore procedures
- Verify ArgoCD can sync complete application state
- Document lessons learned

**Key principle**: Infrastructure as code + GitOps + automated database backups = rapid, reproducible disaster recovery.

## 13. Incident Response

Respond to security incidents in Kubernetes with structured processes for containment and recovery.

### Detection & Initial Response

**Automated Detection**:

- **Prometheus**: Resource anomalies (CPU spikes, pod crashes, restart loops)
- **Trivy Operator**: New critical vulnerabilities in running workloads
- **Kyverno**: Policy violations
- **Kubernetes Audit Logs**: Suspicious API activity (pod exec attempts, secret access)
- **Falco** (if deployed): Runtime threats (shell spawns, privilege escalation, suspicious syscalls)

**Immediate Actions for Pod Compromise**:

1. **Isolate**: Apply NetworkPolicy to block all traffic to/from compromised pod
2. **Preserve**: `kubectl logs pod-name > logs.txt` and `kubectl describe pod pod-name > details.txt`
3. **Terminate**: Delete pod (deployment recreates clean instance)
4. **Investigate**: Analyze logs, Kubernetes audit logs, and runtime alerts for attack vector

### Containment & Recovery

**Emergency Network Isolation**:

Apply default-deny NetworkPolicy to prevent lateral movement:

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: emergency-lockdown
  namespace: compromised-namespace
spec:
  podSelector: {}
  policyTypes:
    - Ingress
    - Egress
  # No rules = blocks all traffic
```

**Recovery Steps**:

- Delete compromised pods (clean instances auto-recreate)
- Rotate secrets in external vault (AWS Secrets Manager, GCP Secret Manager, Azure Key Vault)
- Update container images if CVE was exploited
- Deploy patches via ArgoCD (commit to Git, auto-sync)

### Post-Incident

**Investigation**:

- Collect pod logs, Kubernetes audit logs, Istio service mesh logs
- Review runtime alerts (Falco if deployed, Prometheus anomalies, Kubernetes events)
- Review ArgoCD deployment history and Git commits
- Analyze Trivy vulnerability reports for exploited CVEs

**Documentation & Improvements**:

- Document timeline, attack vector, and remediation actions
- Add Kyverno policies to prevent similar attacks
- Update detection rules (Falco if deployed, Prometheus alerts) to detect similar behaviors earlier
- Notify per compliance requirements (GDPR: 72 hours, HIPAA: 60 days)

## 14. Attack Scenarios Prevented

This guide's security controls prevent real-world Kubernetes attacks commonly seen in production environments.

**Container Escape / Privilege Escalation**

- Attack: Exploiting privileged containers or dangerous capabilities to break out and access host
- Mitigated by: Kyverno blocking privileged containers/capabilities, non-root enforcement, read-only root filesystem, Kubernetes audit logs, runtime detection (Falco if deployed)

**Malicious Runtime Behavior**

- Attack: Unexpected processes spawning (crypto miners, reverse shells, data exfiltration tools)
- Mitigated by: Hardened images with package managers removed (no apt/yum/apk), non-root user enforcement, restrictive NetworkPolicies blocking egress, Istio mTLS preventing lateral movement, Prometheus anomaly detection, runtime monitoring (Falco if deployed)

**Resource Exhaustion / DoS**

- Attack: Malicious/buggy pods consuming all cluster resources causing outages
- Mitigated by: Kyverno requiring CPU/memory limits, ResourceQuotas per namespace, pod disruption budgets, cluster autoscaling

**Lateral Movement via Network Access**

- Attack: Compromised pod used as pivot to attack other pods/services
- Mitigated by: Default-deny NetworkPolicies, Istio mTLS between pods, namespace isolation with explicit allow rules, micro-segmentation

**Database Compromise via Pod Access**

- Attack: Compromised pod used to access and exfiltrate production databases
- Mitigated by: Databases in private subnets with security groups (worker nodes only), credentials in external vaults, application database user with least privilege (non-root), limited permissions on specific tables only, multi-AZ with backups, NetworkPolicies limiting database access

**Control Plane / API Server Attack**

- Attack: Unauthorized access to Kubernetes API to modify cluster or steal secrets
- Mitigated by: Managed Kubernetes hardened control plane, API access restricted to VPN/bastion, RBAC with least privilege, audit logging

**Supply Chain Attack via Unsigned Images**

- Attack: Malicious container images pushed to registry and deployed to production
- Mitigated by: Registry authentication required for push/pull, Kyverno image signature verification (Cosign), image registry allowlist, Trivy Operator continuous scanning, SLSA provenance attestation

**Exploiting Known CVEs in Running Containers**

- Attack: Exploiting publicly disclosed vulnerabilities in outdated images
- Mitigated by: Trivy Operator continuous scanning, alerts on HIGH/CRITICAL with patches, automated image rebuilds with Copacetic, GitOps deployment

**Secrets Exposure in Pod Configs**

- Attack: Secrets leaked through environment variables, ConfigMaps, or insecure Kubernetes Secrets
- Mitigated by: External secrets management (AWS/GCP/Azure vaults), Secrets Store CSI Driver, secrets never in Git/native Secrets, Workload Identity/IRSA

**Compromised ArgoCD / GitOps Repo**

- Attack: Modified GitOps repository to deploy malicious workloads or steal secrets
- Mitigated by: ArgoCD in separate admin cluster, admin access restricted to VPN/bastion, signed Git commits required, RBAC limiting deployment permissions

## 15. References

### Infrastructure & Orchestration

- [Terraform](https://www.terraform.io/)
- [Helm](https://helm.sh/)
- [ArgoCD](https://argo-cd.readthedocs.io/)

### Security & Policy

- [Kyverno](https://github.com/kyverno/kyverno)
- [Trivy Operator](https://github.com/aquasecurity/trivy-operator)
- [Istio](https://github.com/istio/istio)
- [Falco](https://github.com/falcosecurity/falco)
- [Cosign](https://github.com/sigstore/cosign)

### Observability

- [Prometheus](https://prometheus.io/)
- [Grafana](https://grafana.com/)
- [Fluentd](https://github.com/fluent/fluentd)

### Managed Kubernetes Services

- [AWS EKS](https://aws.amazon.com/eks/)
- [GCP GKE](https://cloud.google.com/kubernetes-engine)
- [Azure AKS](https://azure.microsoft.com/en-us/services/kubernetes-service/)

### Standards & Documentation

- [OWASP Kubernetes Security Cheat Sheet](https://cheatsheetseries.owasp.org/cheatsheets/Kubernetes_Security_Cheat_Sheet.html)
- [CIS Kubernetes Benchmark](https://www.cisecurity.org/benchmark/kubernetes)
- [NIST Application Container Security Guide](https://csrc.nist.gov/publications/detail/sp/800-190/final)
- [Kubernetes Security Best Practices](https://kubernetes.io/docs/concepts/security/)
